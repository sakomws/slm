{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37be34fc-0fa9-4811-865c-a3fdc38d38e8",
   "metadata": {},
   "source": [
    "# Fine-tune TinyLlama-1.1B for text-to-SQL generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop module, you will learn how to fine-tune a Llama-based LLM ([TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)) using causal language modelling so that the model learns how to generate SQL queries for text-based instructions. Your fine-tuning job will be launched using SageMaker Training which provides a serverless training environment where you do not need to manage the underlying infrastructure. You will learn how to configure a PyTorch training job using [SageMaker's PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html), and how to leverage the [Hugging Face Optimum Neuron](https://github.com/huggingface/optimum-neuron) package to easily run the PyTorch training job with AWS Trainium accelerators via an [AWS EC2 trn1.2xlarge instance](https://aws.amazon.com/ec2/instance-types/trn1/).\n",
    "\n",
    "For this module, you will be using the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset which consists of thousands of examples of SQL schemas, questions about the schemas, and SQL queries intended to answer the questions.\n",
    "\n",
    "*Dataset example 1:*\n",
    "* *SQL schema/context:* `CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)`\n",
    "* *Question:* `How many departments are led by heads who are not mentioned?`\n",
    "* *SQL query/answer:* `SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)`\n",
    "\n",
    "*Dataset example 2:*\n",
    "* *SQL schema/context:* `CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)`\n",
    "* *Question:* `What are the ids of all students for courses and what are the names of those courses?`\n",
    "* *SQL query/answer:* `SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id`\n",
    "\n",
    "By fine-tuning the model over several thousand of these text-to-SQL examples, the model will then learn how to generate an appropriate SQL query when presented with a SQL context and a free-form question.\n",
    "\n",
    "This text-to-SQL use case was selected so you can successfully fine-tune your model in a reasonably short amount of time (~20 minutes) which is appropriate for this 1hr workshop. Although this is a relatively simple use case, please keep in mind that the same techniques and components used in this module can also be applied to fine-tune LLMs for more advanced use cases such as writing code, summarizing documents, creating blog posts - the possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866074ee-c300-4793-8e63-adbcfc314ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook uses the SageMaker Python SDK to prepare, launch, and monitor the progress of a PyTorch-based training job. Before we get started, it is important to upgrade the SageMaker SDK to ensure that you are using the latest version. Run the next two cells to upgrade the SageMaker SDK and set up your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264aae2-1f18-4b59-a92c-2f169903c202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.12' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Upgrade SageMaker SDK to the latest version\n",
    "%pip install -U sagemaker awscli huggingface_hub ipywidgets -q 2>&1 | grep -v \"warnings/venv\"\n",
    "# Definitely restart your kernel after this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ed574-6db5-471b-8515-c0f6189e653e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\")\n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "region_name=\"us-east-2\" #this is hard coded to a specific region because of Workshop quotas.  You could use sess.boto_region_name\n",
    "sess = Session(boto_session=boto3.Session(region_name=region_name))\n",
    "default_bucket = sess.default_bucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce630d1",
   "metadata": {},
   "source": [
    "This next command just configures the EC2 instance (in us-west-2) to have a default region of us-east-2.  This is specific to the environment in AWS Workshop Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure set region us-east-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a1f57",
   "metadata": {},
   "source": [
    "## Log into Hugging Face\n",
    "\n",
    "The following step is recommended but optional.  If you can log in with your Hugging Face token, it will let you avoid any rate limits for unauthenticated requests.  Even though none of the models or datasets we are using require special permission, if you don't log in your training may fail because of too many unauthenticated requests.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f142253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the cell below stays empty, RESTART YOUR KERNEL if you didn't and run the cells above again\n",
    "# If you can't login in, you can proceed to the next cell.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Uncheck \"Add token as git credential\" or just ignore the error message about it not being added.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22998f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4193108b-25fb-4d3e-85db-c66b8c04c251",
   "metadata": {},
   "source": [
    "## Specify the Optimum Neuron deep learning container (DLC) image\n",
    "\n",
    "The SageMaker Training service uses containers to execute your training script, allowing you to fully customize your training script environment and any required dependencies. For this workshop, you will use a recent Pytorch Training deep learning container (DLC) image which is an AWS-maintained image containing the Neuron SDK and PyTorch.  The Optimum-Neuron library is installed with the requirements.txt file in the assets directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ad886-6977-4295-947b-86d4892b48bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-2.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# Specify the Neuron DLC that we will use for training\n",
    "#   For now, we'll use the standard Neuron DLC and install Optimum Neuron v0.0.27 at training time because we want to use a later SDK \n",
    "#   You can see more about the images here: https://github.com/aws-neuron/deep-learning-containers?tab=readme-ov-file#pytorch-training-neuronx\n",
    "\n",
    "training_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\"\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8802bc-657a-419d-b86d-eb8af5eff90e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure the PyTorch Estimator\n",
    "\n",
    "The SageMaker SDK includes a [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html) class which you can use to define a PyTorch training job that will be executed in the SageMaker managed environment. \n",
    "\n",
    "In the following cell, you will create a PyTorch Estimator which will run the attached `finetune_llama.py` training script on an ml.trn1.2xlarge instance. The `finetune_llama.py` script is an Optimum Neuron training script that can be used for causal language modelling with AWS Trainium. The scripts will be downloaded as the instance is brought up, and the scripts will download the model and the datasets onto the SageMaker training instance.\n",
    "\n",
    "The PyTorch Estimator has many parameters that can be used to configure your training job. A few of the most important parameters include:\n",
    "\n",
    "- *entry_point*: refers to the name of the training script that will be executed as part of this training job\n",
    "- *source_dir*: the path to the local source code directory (relative to your notebook) that will be packaged up and included inside your training container\n",
    "- *instance_count*: defines how many EC2 instances to use for this training job\n",
    "- *instance_type*: determines which type of EC2 instance will be used for training\n",
    "- *image_uri*: defines which training DLC will be used to run the training job (see Neuron DLC, above)\n",
    "- *distribution*: determines which type of distribution to use for the training job - you will need 'torch_distributed' for this workshop\n",
    "- *environment*: provides a dictionary of environment variables which will be applied to your training environment\n",
    "- *hyperparameters*: provides a dictionary of command-line arguments to pass to your training script, ex: finetune_llama.py\n",
    "\n",
    "In the `hyperparameters` section, you can see the specific command-line arguments that are used to control the behavior of the `finetune_llama.py` training script. Notably:\n",
    "- *model_id*: specifies which model you will be fine-tuning, in this case a recent checkpoint from the TinyLlama-1.1B project\n",
    "- *tokenizer_id*: specifies which tokenizer you will used to tokenize the dataset examples during training\n",
    "- *output_dir*: directory in which the fine-tuned model will be saved. Here we use the SageMaker-specific `/opt/ml/model` directory. At the end of the training job, SageMaker automatically copies the contents of this directory to the output S3 bucket\n",
    "- *tensor_parallel_size*: the tensor parallel degree for which we want to use for training. In this case we use '2' to shard the model across the 2 NeuronCores available in the trn1.2xlarge instance\n",
    "- *bf16*: request BFloat16 training\n",
    "- *per_device_train_batch_size*: the microbatch size to be used for fine-tuning\n",
    "- *gradient_accumulation_steps*: how many steps for which gradients will be accumulated between updates\n",
    "- *max_steps*: the maximum number of steps of fine-tuning that we want to perform\n",
    "- *lora_r*, *lora_alpha*, *lora_dropout*: the LoRA rank, alpha, and dropout values to use during fine-tuning\n",
    "\n",
    "The below estimator has been pre-configured for you, so you do not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the hyperparameters are command-line args passed to the finetune_llama.py script to control its behavior\n",
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"tokenizer_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"skip_cache_push\": True,\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"tensor_parallel_size\": 2,\n",
    "    \"bf16\": True,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"max_steps\": 1000,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"logging_steps\": 10,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"dataloader_drop_last\": True,\n",
    "    \"disable_tqdm\": True,\n",
    "}\n",
    "\n",
    "# Set up environment variables\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "environment = {\"FI_EFA_FORK_SAFE\": \"1\", \"WANDB_DISABLED\": \"true\"}\n",
    "token = HfFolder.get_token()\n",
    "if token is not None:\n",
    "    environment[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Set up the PyTorch estimator\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"finetune_llama.py\",\n",
    "    source_dir=\"./assets\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.2xlarge\",\n",
    "    disable_profiler=True,\n",
    "    output_path=f\"s3://{default_bucket}/neuron_events2025\",\n",
    "    base_job_name=\"trn1-tinyllama\",\n",
    "    sagemaker_session=sess,\n",
    "    code_bucket=f\"s3://{default_bucket}/neuron_events2025_code\",\n",
    "    checkpoint_s3_uri=f\"s3://{default_bucket}/neuron_events_output\",\n",
    "    image_uri=training_image,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment=environment,\n",
    "    disable_output_compression=True,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278940b-f563-4582-9df0-bd56d9b5fd28",
   "metadata": {},
   "source": [
    "## Launch the training job\n",
    "\n",
    "Once the estimator has been created, you can then launch your training job by calling `.fit()` on the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7829c64-0190-43c3-be1a-0ccce7d45248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: trn1-tinyllama-2025-05-13-00-40-31-750\n"
     ]
    }
   ],
   "source": [
    "# Call fit() on the estimator to initiate the training job\n",
    "pt_estimator.fit(wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77434b2-94d7-4256-8d0b-d5d2ddb1d5ae",
   "metadata": {},
   "source": [
    "## Monitor the training job\n",
    "\n",
    "When the training job has been launched, the SageMaker Training service will then take care of:\n",
    "- launching and configuring the requested EC2 infrastructure for your training job\n",
    "- launching the requested container image on each of the EC2 instances\n",
    "- copying your source code directory and running your training script within the container(s)\n",
    "- storing your trained model artifacts in Amazon Simple Storage Service (S3)\n",
    "- decommissioning the training infrastructure\n",
    "\n",
    "While the training job is running, the following cell will periodically check and output the job status. When you see 'Completed', you know that your training job is finished and you can proceed to the remainder of the notebook. The training job typically takes about 20 minutes to complete.\n",
    "\n",
    "If you are interested in viewing the output logs from your training job, you can view the logs by navigating to the AWS CloudWatch console, selecting `Logs -> Log Groups` in the left-hand menu, and then looking for your SageMaker training job in the list. **Note:** it will usually take 4-5 minutes before the infrastructure is running and the output logs begin to be populated in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c223037-2f8e-4eb0-9e4b-ff4dac6ede7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13T00:40:37.718399 Training job status: InProgress!\n",
      "2025-05-13T00:41:07.827456 Training job status: InProgress!\n",
      "2025-05-13T00:41:37.941892 Training job status: InProgress!\n",
      "2025-05-13T00:42:08.055514 Training job status: InProgress!\n",
      "2025-05-13T00:42:38.170184 Training job status: InProgress!\n",
      "2025-05-13T00:43:08.285526 Training job status: InProgress!\n",
      "2025-05-13T00:43:38.401669 Training job status: InProgress!\n",
      "2025-05-13T00:44:08.517601 Training job status: InProgress!\n",
      "2025-05-13T00:44:38.607279 Training job status: InProgress!\n",
      "2025-05-13T00:45:08.901240 Training job status: InProgress!\n",
      "2025-05-13T00:45:39.029987 Training job status: InProgress!\n",
      "2025-05-13T00:46:09.148483 Training job status: InProgress!\n",
      "2025-05-13T00:46:39.262424 Training job status: InProgress!\n",
      "2025-05-13T00:47:09.378729 Training job status: InProgress!\n",
      "2025-05-13T00:47:39.477011 Training job status: InProgress!\n",
      "2025-05-13T00:48:09.589262 Training job status: InProgress!\n",
      "2025-05-13T00:48:39.715998 Training job status: InProgress!\n",
      "2025-05-13T00:49:09.833712 Training job status: InProgress!\n",
      "2025-05-13T00:49:40.132350 Training job status: InProgress!\n",
      "2025-05-13T00:50:10.259671 Training job status: InProgress!\n",
      "2025-05-13T00:50:40.376526 Training job status: InProgress!\n",
      "2025-05-13T00:51:10.492630 Training job status: InProgress!\n",
      "2025-05-13T00:51:40.612684 Training job status: InProgress!\n",
      "2025-05-13T00:52:10.735871 Training job status: InProgress!\n",
      "2025-05-13T00:52:40.856541 Training job status: InProgress!\n",
      "2025-05-13T00:53:10.978185 Training job status: InProgress!\n",
      "2025-05-13T00:53:41.102406 Training job status: InProgress!\n",
      "2025-05-13T00:54:11.391318 Training job status: InProgress!\n",
      "2025-05-13T00:54:41.506542 Training job status: InProgress!\n",
      "2025-05-13T00:55:11.619419 Training job status: InProgress!\n",
      "2025-05-13T00:55:41.736144 Training job status: InProgress!\n",
      "2025-05-13T00:56:11.850643 Training job status: InProgress!\n",
      "2025-05-13T00:56:41.965740 Training job status: InProgress!\n",
      "2025-05-13T00:57:12.082235 Training job status: InProgress!\n",
      "2025-05-13T00:57:42.193146 Training job status: InProgress!\n",
      "2025-05-13T00:58:12.309523 Training job status: InProgress!\n",
      "2025-05-13T00:58:42.596288 Training job status: InProgress!\n",
      "2025-05-13T00:59:12.715701 Training job status: InProgress!\n",
      "2025-05-13T00:59:42.835134 Training job status: InProgress!\n",
      "2025-05-13T01:00:12.952002 Training job status: InProgress!\n",
      "2025-05-13T01:00:43.070275 Training job status: InProgress!\n",
      "2025-05-13T01:01:13.187416 Training job status: InProgress!\n",
      "2025-05-13T01:01:43.291955 Training job status: InProgress!\n",
      "\n",
      "2025-05-13T01:02:13.412501 Training job status: Completed!\n"
     ]
    }
   ],
   "source": [
    "# Periodically check job status until it shows 'Completed' (ETA ~20 minutes)\n",
    "#  You can also monitor job status in the SageMaker console, and view the\n",
    "#  SageMaker Training job logs in the CloudWatch console\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "while (job_status := pt_estimator.jobs[-1].describe()['TrainingJobStatus']) not in ['Completed', 'Error', 'Failed']:\n",
    "    print(f\"{datetime.now().isoformat()} Training job status: {job_status}!\")\n",
    "    sleep(30)\n",
    "\n",
    "print(f\"\\n{datetime.now().isoformat()} Training job status: {job_status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c94343-b0c6-4903-82cc-c8ab2f88b26b",
   "metadata": {},
   "source": [
    "## Determine location of fine-tuned model artifacts\n",
    "\n",
    "Once the training job has completed, SageMaker will copy your fine-tuned model artifacts to a specified location in S3.\n",
    "\n",
    "In the following cell, you can see how to programmatically determine the location of your model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213af977-8ed6-4081-af65-59c70db2dbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your fine-tuned model is available here:\n",
      "\n",
      "s3://this.output.should.be.replaced.with.a.real.s3.path.once.the.cell.is.executed/\n"
     ]
    }
   ],
   "source": [
    "# Show where the fine-tuned model is stored - previous job must be 'Completed' before running this cell\n",
    "model_archive_path = pt_estimator.jobs[-1].describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Your fine-tuned model is available here:\\n\\n{model_archive_path}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f529f-a548-4fbd-b160-3cab5f52c488",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Note:** Please copy the above S3 path, as it will be required in the subsequent workshop module.\n",
    "\n",
    "\n",
    "Lastly, run the following cell to list the model artifacts available in your S3 model_archive_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ad8c7e-6a73-4f20-944f-ac12ef286a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:01:39        714 config.json\n",
      "2025-05-13 01:01:48        124 generation_config.json\n",
      "2025-05-13 01:01:40 4400216536 model.safetensors\n",
      "2025-05-13 01:01:47        551 special_tokens_map.json\n",
      "2025-05-13 01:01:47    1842795 tokenizer.json\n",
      "2025-05-13 01:01:39     499723 tokenizer.model\n",
      "2025-05-13 01:01:48       1368 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# View the contents of the fine-tuned model path in S3\n",
    "!aws s3 ls {model_archive_path}/merged_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9ffa7-a694-48c0-acde-cd468d18a448",
   "metadata": {},
   "source": [
    "Congratulations on completing the LLM fine-tuning module!\n",
    "\n",
    "In the next notebook, you will learn how to deploy your fine-tuned model in a SageMaker hosted endpoint, and leverage AWS Inferentia accelerators to perform model inference. Have fun!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
