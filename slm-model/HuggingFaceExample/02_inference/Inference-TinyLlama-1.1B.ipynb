{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74156eb6-9517-475f-984e-2e76d24fb281",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy a fine-tuned TinyLlama-1.1B model for text-to-SQL inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop module, you will learn how to deploy a Large Language Model (LLM) to [Amazon EC2 inf2 instance](https://aws.amazon.com/ec2/instance-types/inf2/) for generative AI inference.\n",
    "You will use Amazon SageMaker with [Hugging Face TGI images specific for Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/containers/locate-neuron-dlc-image.html) to deploy the model fine-tuned in the previous workshop module. Amazon SageMaker Hosting provides fully managed options for deploying our models for Real-Time or Batch inference modes. AWS Inferentia provides the best cost per inference.\n",
    "\n",
    "This workbook assumes that you have previously run the Finetune-TinyLlama-1.1B module and you have copied the s3 path for the finetuned model.  If you didn't complete that for some reason (and we recommend you do), you can still deploy a copy of the same finetuned model that we posted on Hugging Face at aws-neuron/NeuronWorkshop2025 .  You'll still need to run the Prerequisites section, skip the Compilation, and change the HF_MODEL_ID and comment out the S3 path in the Create SageMaker Endpoint section. (There are comments to show you what to change)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc71b2-88a2-48bc-919c-9a9f86c419ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook uses the SageMaker Python SDK to deploy a fine-tuned model using SageMaker hosting service. Before we get started, it is important to upgrade the SageMaker SDK to ensure that you are using the latest version. Run the next two cells to upgrade the SageMaker SDK and set up your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bea784-0ea8-47dc-9010-da20fa55e4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrade SageMaker SDK to the latest version\n",
    "%pip install -U sagemaker awscli -q 2>&1 | grep -v \"warnings/venv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c56a4c",
   "metadata": {},
   "source": [
    "This next command just configures the EC2 instance (in us-west-2) to have a default region of us-east-2.  This is specific to the environment in AWS Workshop Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63686789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just in case you didn't run it in the fine-tune notebook\n",
    "!aws configure set region us-east-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee34f8d-2912-4097-a0ad-8161bdbaa2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging \n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\") \n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, utils\n",
    "import boto3\n",
    "\n",
    "# NOTE: We currently need to use us-east-2 for model deployment when running this notebook in an AWS Workshop Studio event.\n",
    "boto3_sess = boto3.Session(region_name=\"us-east-2\")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session = boto3_sess)  # sagemaker session for interacting with different AWS APIs\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62083e2-b9dc-4f61-96f2-8a00056d19ce",
   "metadata": {},
   "source": [
    "## Specify the Hugging Face container image\n",
    "\n",
    "[SageMaker hosting containers for Inferentia and Trainium](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/containers/locate-neuron-dlc-image.html) use the [Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) to support the NeuronCores on Inferentia and Trainium devices. The [Hugging Face TGI server project](https://github.com/huggingface/text-generation-inference) supports both GPUs and Neuron devices. A version of that server that supports SageMaker and Neuron can be found with the get_huggingface_llm_image_uri command in the SageMaker SDK.  In this case, we supply the server type (huggingface-neuronx) along with the region and Optimum Neuron version number.\n",
    "\n",
    "\n",
    "This image facilitates the loading of models onto [AWS Inferentia2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inferentia2.html) accelerators, parallelizes the model across multiple [NeuronCores](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch), and enables serving via HTTP endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff5e52d-2b6d-4e6e-b645-18c4174a8ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.28-neuronx-py310-ubuntu22.04'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    \"huggingface-neuronx\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.0.28\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8fc7a-1608-4fa7-9799-34d4bbdcb09d",
   "metadata": {},
   "source": [
    "## Compiling the model for Neuron\n",
    "\n",
    "The TGI container expects either a model that has been compiled for Neuron or a reference to a model architecture that is stored in the [Optimum Neuron model cache](https://github.com/huggingface/optimum-neuron/blob/main/docs/source/guides/cache_system.mdx)\n",
    "\n",
    "We will do that with a second training job in SageMaker.  It is important that the image_uri you use for compilation is the same as what you will use for hosting.  (this may be a different URI than you used for training)\n",
    "\n",
    "What the training job does is call the optimum-cli command in the image with the model path as well as the parameters for compilation.  See the [Optimum-Neuron documentation](https://github.com/huggingface/optimum-neuron/blob/main/docs/source/guides/export_model.mdx#exporting-a-model-to-neuron-using-the-cli) for more details.\n",
    "\n",
    "In the following cell, you will need to update *`s3_orig_model_path`* with the S3 path you copied from the previous workshop module where fine-tuned model artifact is available. It should be something like\n",
    "```\n",
    "s3_orig_model_path=\"s3://sagemaker-us-east-2-xxxxxxxxxxxx/neuron_events2025/trn1-tinyllama-2024-12-xx-xx-xx-xx-xxx/output/model/\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a3c0e-556e-4f0f-b5eb-e9259bebc0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_orig_model_path=\"\"  # <- change this path to your S3 model path from the Finetune notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797454",
   "metadata": {},
   "source": [
    "The settings in the container_arguments below for sequence length, batch size, and number of cores must match the settings you will use in your hub environment variables later.  The version of the SDK and Optimum Neuron must match as well, but we ensure that by using the same container for both compilation as well as hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f1220-a939-4779-86b2-15976f76348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_output_path s3://sagemaker-us-east-2-445365384523/neuron_events2025/trn1-tinyllama-2025-05-13-00-40-31-750/output/model/compiled_model/\n",
      "s3_model_path s3://sagemaker-us-east-2-445365384523/neuron_events2025/trn1-tinyllama-2025-05-13-00-40-31-750/output/model/merged_model/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameters\n",
    "s3_output_path=f\"{s3_orig_model_path}compiled_model/\"\n",
    "print(\"s3_output_path\",s3_output_path)\n",
    "training_job_name = utils.name_from_base(\"TGICompilation\")\n",
    "print(\"training_job_name\")\n",
    "s3_model_path = f\"{s3_orig_model_path}merged_model/\"\n",
    "print(\"s3_model_path\",s3_model_path)\n",
    "\n",
    "container_entrypoint = [\"optimum-cli\"]\n",
    "container_arguments = [\"export\", \"neuron\", \"--model\", \"/opt/ml/input/data/modeldir/\", \"--task\", \"text-generation\", \"--sequence_length\", \"512\", \"--batch_size\", \"1\", \"--num_cores\", \"2\", \"/opt/ml/output/data/\"]\n",
    "\n",
    "input_data_config = {\n",
    "    \"ChannelName\": \"modeldir\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": s3_model_path\n",
    "        }\n",
    "    }\n",
    "}\n",
    "output_data_config = {\n",
    "    \"S3OutputPath\": s3_output_path\n",
    "}\n",
    "resource_config = {\n",
    "    \"VolumeSizeInGB\": 20,\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.trn1.2xlarge\"\n",
    "}\n",
    "stopping_condition = {\n",
    "    \"MaxRuntimeInSeconds\": 1800\n",
    "}\n",
    "\n",
    "# Create the SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', region_name=sess.boto_session.region_name)\n",
    "\n",
    "# Create the training job\n",
    "response = sagemaker.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    RoleArn=role,\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingInputMode': 'File',\n",
    "        'TrainingImage': image_uri,\n",
    "        'ContainerEntrypoint': container_entrypoint,\n",
    "        'ContainerArguments': container_arguments\n",
    "    },\n",
    "    InputDataConfig=[input_data_config],\n",
    "    OutputDataConfig=output_data_config,\n",
    "    ResourceConfig=resource_config,\n",
    "    StoppingCondition=stopping_condition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be26b70",
   "metadata": {},
   "source": [
    "Just like before, this code will check on the status of the training job used for compilation every 30 seconds.  It should take 5-6 minutes for the compilation job to finish.  (This is pulling from the [Neuron Model cache](https://huggingface.co/docs/optimum-neuron/en/guides/cache_system).  If you are using this code to compile a different model, it could take longer to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b6c85-b577-4756-80ad-91b6f6d12a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13T01:06:59.279708 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:07:29.389634 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:07:59.478368 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:08:29.596501 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:08:59.708566 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:09:29.824667 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:09:59.932030 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "2025-05-13T01:10:30.049864 Training job TGICompilation-2025-05-13-01-04-42-090 status: InProgress!\n",
      "\n",
      "2025-05-13T01:11:00.199997 Training job status: Completed!\n"
     ]
    }
   ],
   "source": [
    "# Periodically check job status until it shows 'Completed' (ETA ~6 minutes)\n",
    "#  You can also monitor job status in the SageMaker console, and view the\n",
    "#  SageMaker Training job logs in the CloudWatch console\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "while (job_status := sess.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']) not in ['Completed', 'Error', 'Failed']:\n",
    "    print(f\"{datetime.now().isoformat()} Training job {training_job_name} status: {job_status}!\")\n",
    "    sleep(30)\n",
    "\n",
    "print(f\"\\n{datetime.now().isoformat()} Training job status: {job_status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df241f7-f8f3-4057-9c97-b8898371363d",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint\n",
    "Next, we create the SageMaker endpoint with the model configuration defined earlier. We use the `ml.inf2.xlarge` instance containing a single Inferentia2 accelerator with 2 NeuronCores. Model deployment will usually take 4-5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ae159-d05b-4657-bc38-c1c72bd1568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model/\",\n",
    "    #\"HF_MODEL_ID\": \"aws-neuron/NeuronWorkshop2025\", #You only need to use this if you didn't successfully train and compile the model\n",
    "    \"HF_NUM_CORES\": \"2\",\n",
    "    \"HF_AUTO_CAST_TYPE\": \"bf16\",\n",
    "    \"MAX_BATCH_SIZE\": \"1\",\n",
    "    \"MAX_INPUT_LENGTH\": \"500\",\n",
    "    \"MAX_TOTAL_TOKENS\": \"512\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952610a-1f79-40ad-9465-b5d9fc9ad3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "s3_new_model_path = f\"{s3_output_path}{training_job_name}/output/output.tar.gz\"\n",
    "\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session = sess,\n",
    "    model_data=s3_new_model_path #comment out this line if you are using the aws-neuron/NeuronWorkshop2025 model directly from Hugging Face\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4c616d-2b87-4d8c-a66b-8ae2399c57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name tinyllama-finetuned-model-2025-05-13-01-11-44-328\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.inf2.xlarge\"\n",
    "endpoint_name = utils.name_from_base(\"tinyllama-finetuned-model\")\n",
    "print(\"endpoint_name\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481284b8",
   "metadata": {},
   "source": [
    "*`You can ignore the message that says \"Your model is not compiled. Please compile your model before using Inferentia.\"`*\n",
    "\n",
    "It should take 6-7 minutes to deploy the endpoint.  You will know it is done when you see an exclamation point at the end of the dashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e8545e9-d116-49f6-933b-5cc5709d581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=500,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a226-3178-45a6-a2fb-9f7bec510370",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference tests\n",
    "After the SageMaker endpoint has been created, we can make real-time predictions against SageMaker endpoints using the Predictor object:\n",
    "- Create a predictor for submit inference requests and receive responses\n",
    "- Responses include the initial request\n",
    "\n",
    "Keep in mind that this is a small model that has only been trained for 1000 steps, so while the responses should be formatted as SQL, they might not quite be what is expected.  See the optional section below that includes output from the original (not fine tuned) model and you can see it is more conversational."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977b17d-dabc-45ab-89e9-43621f9ac7fe",
   "metadata": {},
   "source": [
    "Let's submit a few inference requests to the model server and display the inference results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f7a756-0d10-4c5b-b8a5-b4a05e4e423c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=\"\"\"\n",
    "<|system|>\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\n",
    "<|user|>\n",
    "How many departments are led by heads who are not mentioned?</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92dbb2-a13c-47b9-bb71-cc0bce45eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\n",
      "<|user|>\n",
      "How many departments are led by heads who are not mentioned?</s>\n",
      "<|assistant|>\n",
      "SELECT COUNT(*) FROM management WHERE department_id NOT IN (SELECT dept_id FROM department WHERE NOT head);</s>\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "   \"inputs\": example\n",
    "}\n",
    "\n",
    "result = predictor.predict(data)\n",
    "\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e517d8ca-a007-4d6c-82ca-e903e9e7dc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=\"\"\"\n",
    "<|system|>\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE \n",
    "student_course_registrations (student_id VARCHAR, course_id VARCHAR)</s>\n",
    "<|user|>\n",
    "What are the ids of all students for courses and what are the names of those courses?</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0021ab-8428-4851-88f3-882a75d702e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE \n",
      "student_course_registrations (student_id VARCHAR, course_id VARCHAR)</s>\n",
      "<|user|>\n",
      "What are the ids of all students for courses and what are the names of those courses?</s>\n",
      "<|assistant|>\n",
      "SELECT student_id FROM student_course_registrations JOIN courses ON student_course_registrations.course_id = courses.course_id WHERE courses.course_name = \"Courses\";</s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "result = predictor.predict(\n",
    "    {\"inputs\": example, \"parameters\": {\"do_sample\": True,\"max_new_tokens\": 100,\"temperature\": 0.7,\"watermark\": True}}\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e35f617-4721-419d-8d23-78fb6dee5d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=\"\"\"\n",
    "<|system|>\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)</s>\n",
    "<|user|>\n",
    "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91c03b95-81c4-4907-8e22-cf15e385b920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)</s>\n",
      "<|user|>\n",
      "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?</s>\n",
      "<|assistant|>\n",
      "SELECT MAX(wins) FROM table_name_9 WHERE team = \"kawasaki\" AND points = \"95\" AND year < 1981;</s>\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(\n",
    "    {\"inputs\": example, \"parameters\": {\"do_sample\": True,\"max_new_tokens\": 100,\"temperature\": 0.7,\"watermark\": True}}\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37c4b5-8c22-42e8-8509-00c520b7cc8f",
   "metadata": {},
   "source": [
    "## Cleanup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db708703-2952-405f-8e47-c026cc055448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "#model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad8af2-1289-4b6c-ae4d-59eafa0eeb50",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations on completing the LLM deployment for the inference module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d3363-d20c-4911-83dd-8d1e619024e8",
   "metadata": {},
   "source": [
    "## (Optional) Deploy original TinyLlama model from Hugging Face hub\n",
    "\n",
    "If you have spare time, you can also consider deploying the original TinyLlama model from [Hugging Face hub](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4) for even more fun !\n",
    "\n",
    "In this scenario, you can specify the name of the Hugging Face model using the *`model_id`* parameter to download the model directly from the Hugging Face repo. The remaining steps of the process remain the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c765a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\") \n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, utils\n",
    "import boto3\n",
    "\n",
    "# NOTE: We currently need to use us-east-2 for model deployment when running this notebook in an AWS Workshop Studio event.\n",
    "boto3_sess = boto3.Session(region_name=\"us-east-2\")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session = boto3_sess)  # sagemaker session for interacting with different AWS APIs\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ccd14a7-e172-48fb-987d-5100f1a08f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.28-neuronx-py310-ubuntu22.04'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    \"huggingface-neuronx\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.0.28\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec381ba-6a96-4b9e-9ee0-bdcca0fe12e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hub = {\n",
    "    \"HF_MODEL_ID\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    #\"HF_MODEL_ID\": \"aws-neuron/NeuronWorkshop2025\", # this is the fine tuned model you would get if you ran the Finetune-TinyLlama-1.1B notebook\n",
    "    \"HF_NUM_CORES\": \"2\",\n",
    "    \"HF_AUTO_CAST_TYPE\": \"bf16\",\n",
    "    \"MAX_BATCH_SIZE\": \"1\",\n",
    "    \"MAX_INPUT_LENGTH\": \"500\",\n",
    "    \"MAX_TOTAL_TOKENS\": \"512\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517f086-9ed2-47a5-b682-5b8685dc14f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    sagemaker_session = sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54bbfc6e-67f0-425e-bb49-36953ae4d191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name tinyllama-finetuned-model-2025-05-13-01-35-06-102\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.inf2.xlarge\"\n",
    "endpoint_name = utils.name_from_base(\"tinyllama-finetuned-model\")\n",
    "print(\"endpoint_name\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89252eb3",
   "metadata": {},
   "source": [
    "This next cell may take 5-6 minutes to run while the endpoint is deploying.  \n",
    "\n",
    "*`You can ignore the message that says \"Your model is not compiled. Please compile your model before using Inferentia.\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2b7f4fc-2d7d-4440-b2d0-9644a9065338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=500,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79f85d5a-1df9-49bb-969a-6959c2faf37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=\"\"\"\n",
    "<|system|>\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\n",
    "<|user|>\n",
    "How many departments are led by heads who are not mentioned?</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e958823-a660-4ab4-a377-edefdd31bfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\n",
      "<|user|>\n",
      "How many departments are led by heads who are not mentioned?</s>\n",
      "<|assistant|>\n",
      "There is no information provided in the given text that suggests the number of departments led by heads who are not mentioned.</s>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "result = predictor.predict(\n",
    "    {\"inputs\": example, \"parameters\": {\"do_sample\": True,\"max_new_tokens\": 100,\"temperature\": 0.7,\"watermark\": True}}\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed84cad1-45c6-4b97-8401-5c5f63564bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "#model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6363f-3b84-4ac2-9aaf-2a42af699a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
