{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install vLLM\n",
    "\n",
    "There is a prior version of the SDK that was upstreamed into the main vLLM repository.  However, most of the time we want to install from source from the aws-neuron fork.  \n",
    "\n",
    "Instructions are available here:  https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide.html#nxdi-vllm-user-guide\n",
    "\n",
    "However, the steps are below.  Run the next three cells.  The pip installs could take 5 minutes.\n",
    "\n",
    "The AWS workshop environment deploys using a Neuron DLAMI with a recent SDK.  If you are deploying this in your own environment, you may need to match the branch to your SDK version or follow the latest instructions at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone -b 2.25.0 https://github.com/aws-neuron/upstreaming-to-vllm.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -r /home/ubuntu/environment/vLLM/upstreaming-to-vllm/requirements/neuron.txt\n",
    "#expected to produce no output for 4 or 5 minutes.  Remove the --quiet flag if you want to see ALL the packages installed!  Or look in the neuron.txt requirements doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!VLLM_TARGET_DEVICE=\"neuron\" pip install --quiet -e /home/ubuntu/environment/vLLM/upstreaming-to-vllm/.\n",
    "# expected to product no output for 5 or 6 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download copies of the model to deploy\n",
    "We are downloading a copy of the stock Qwen3-8B model as well as the compiled version from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download aws-neuron/Qwen3-8BSharded --local-dir /home/ubuntu/environment/qwen3\n",
    "#this could take 3-4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download Qwen/Qwen3-8B --local-dir /home/ubuntu/environment/Qwen3-8B --exclude \"*.safetensors\"\n",
    "#This is the stock model.  It will only take seconds because we don't need to download the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure you restart your kernel\n",
    "If you get an error that vllm could not be found, it is because you didn't restart your kernel after installing it above\n",
    "\n",
    "# Offline inference example\n",
    "\n",
    "In this example, we load the qwen3 precompiled model artifacts (or NEFF files) and the model presharded for two cores.  We do this because of the system memory limitations of the trn1.2xlarge (32GB of system RAM).  The trn1.2xlarge also has 32GB of device RAM on the Trainium1 device (that has two Neuron cores), but system RAM is (usually) our limiter for compiling.\n",
    "\n",
    "May take 8 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "os.environ['VLLM_NEURON_FRAMEWORK'] = \"neuronx-distributed-inference\"\n",
    "os.environ['NEURON_COMPILED_ARTIFACTS'] = \"/home/ubuntu/environment/qwen3\"\n",
    "#os.environ['BASE_COMPILE_WORK_DIR'] = \"/home/ubuntu/qwen3/\"\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/Qwen3-8B\", #model weights\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=1024,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# note that top_k must be set to lower than the global_top_k defined in\n",
    "# the neuronx_distributed_inference.models.config.OnDeviceSamplingConfig\n",
    "sampling_params = SamplingParams(top_k=10, temperature=0.8, top_p=0.95)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "# Free up the Neuron Cores for the next step -- in production, keep the object around to avoid load times and warmup times\n",
    "del llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online inference example.\n",
    "In this case, we are loading the model directly from Hugging Face and compiling what we need as we go (this is a 1.1B parameter model, so it needs less system RAM to compile than the Qwen3-8B example above)\n",
    "It may take 5 minutes for the model to download, compile and run.\n\n",
    "# Restart your kernel!!\n",
    "Restart your kernel before you run the next cell.  This will remove the python script and anything it has loaded in the devices.\n"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you are running this in a Jupyter notebook, this cell will keep running until you stop it.  The server should remain available (and using the Neuron cores) until you stop it.  \n",
    "\n",
    "You'll run this cell with different parameters your instructor will be discussing and using the guidellm tool in the Benchmark.ipynb notebook to run against this server.\n",
    "\n",
    "Run the next cell and wait until you see something like this (it should take about 5 minutes):\n",
    "```\n",
    "INFO:     Started server process [21298]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!VLLM_NEURON_FRAMEWORK='neuronx-distributed-inference' python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \\\n",
    "    --max-num-seqs=1 \\\n",
    "    --max-model-len=1024 \\\n",
    "    --tensor-parallel-size=2 \\\n",
    "    --port=8080 \\\n",
    "    --device \"neuron\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
