{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b1e73f-dc2c-4d66-b3ba-4fb71b5243c8",
   "metadata": {},
   "source": [
    "# Write your own kernel with the Neuron Kernel Interface (NKI)\n",
    "In this notebook you'll learn how to develop your own kernel with [NKI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html). A kernel is a set of user-defined functions that are executed largely as defined by the user, not by the compiler. With NKI you can write your own functions to define any operations you like, using supported APIs, and execute them on Trainium and Inferentia hardware. You have the control and lower-level access to define the data movement, computational patterns, and physical execution for the mathematics of your algorithms with NKI.\n",
    "\n",
    "The structure of the notebook is as follows:\n",
    "1. Brief introduction to the NeuronCore and the NKI programming model\n",
    "2. Your first NKI kernel - tensor addition\n",
    "3. Your second NKI kernel - matrix multiplication\n",
    "\n",
    "Wrap up and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d843c8-b824-4896-ad23-1098dd859872",
   "metadata": {},
   "source": [
    "### 1. Introduction to the NeuronCore and NKI programming model\n",
    "The NeuronCore is the main acceleration unit within AWS AI chips Trainium and Inferentia. As you can see in the image below, it is composed of 4 compute engines. These engines are based on a systollic array architecture. The compute engines are fed data from the primary on-chip memory cache, SBUF. Data is moved from the HBM banks to SBUF when you call `nl.load`. You'll index into your tensors to create lower-level objects, called `tiles`. A tile is the result of `nl.load`. Once you've defined `tiles`, you can send them to various NKI mathematical APIS such as `add`, `subtract`, `matmul`, etc. The result of these operations are stored on the secondary on-chip memory cache, PSUM. After moving the data back to SBUF, you can then send it back to HBM with `nl.store`.\n",
    "\n",
    "<img src=https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/pm-nc.png width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc461d-1067-4b96-95a5-3da49e15723f",
   "metadata": {},
   "source": [
    "Trainium1 chips feature two NeuronCore-v2 acceleration units, 2 HBM banks, NeuronLink-v2 chip-to-chip connect, host PCIE, and dedicated engines for both data movement and collective communications. Trainium1 offers 32 GiB of device memory (sum of all 4 HBM banks), with 840 GiB/sec of bandwidth. Trainium1 instances feature 16 Trainium chips, providing a total of up to 3 petaflops of FP16 compute and 512 accelerator memory capacity. For more architectural details, see our docs [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/trainium.html#trainium-arch). \n",
    "\n",
    "\n",
    "The on-chip memory cache, SBUF, **has ~20x higher memory bandwidth than HBM**. The purpose of your kernel is to exploit as much of that compute acceleration as you can within the context of your model and workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffc189-2875-4a42-a14a-de4ea122d2ef",
   "metadata": {},
   "source": [
    "#### Structuring data and tensors for NKI\n",
    "\n",
    "To easily move data and design our kernels on NKI, we'll want to exploit the 128 partitions built into SBUF as shown in the image below. In particular, SBUF has 128 partition lanes. Each of these lanes can execute programs in parallel on the engines. As much as possible, we'll want to align the tensors and data structures in our algorithms to follow this physical design. The benefit is that our kernels will run faster and be easier to develop!\n",
    "\n",
    "Your data movement from HBM to SBUF should be very carefully aligned with this 128-lane partition dimension, also called p-dim. Each tile needs a precise definition along the p-dim. Your second dimension is called the free dimension, or f-dim. As the name goes, this dimension is much more flexible than p-dim. Though it may surprise you, it's better not to fully saturate sbuf with extremely large tiles. This is so that the compiler can overlap data movement and collectives with compute, giving you better overall compute utilization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c786fce-ac8e-4549-8bf1-edaee7512211",
   "metadata": {},
   "source": [
    "<img src=https://awsdocs-neuron.readthedocs-hosted.com/en/latest/_images/pm-layout.png width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9fbbe-2fa4-41c0-9cc8-7560fbc7a49f",
   "metadata": {},
   "source": [
    "### 2. Your first NKI kernel\n",
    "Now that you have some understanding of the compute architecture and motivation for kernels, let's write your first NKI kernel! Importing the `nki` library may take a few moments the first time you've imported it on an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da52760-db72-403a-ade9-d8bebac40de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neuronxcc.nki as nki\n",
    "import neuronxcc.nki.language as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83039ee-1788-478f-809f-f139cb032cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nki.jit\n",
    "def nki_tensor_add_kernel_(a_input, b_input):\n",
    " \n",
    "  # Create output tensor \n",
    "  c_output = nl.ndarray(a_input.shape, dtype=a_input.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "  # Load input data from device memory (HBM) to on-chip memory (SBUF)\n",
    "  a_tile = nl.load(a_input)\n",
    "  b_tile = nl.load(b_input)\n",
    "\n",
    "  # compute a + b\n",
    "  c_tile = a_tile + b_tile\n",
    "\n",
    "  # return the final tensor\n",
    "  nl.store(c_output, value=c_tile)\n",
    "\n",
    "  # Transfer the ownership of `c_output` to the caller\n",
    "  return c_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486f0e0a-6af1-4882-afe2-4ce5a1912ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NKI and NumPy match\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(128, 512).astype(np.float16)\n",
    "b = np.random.rand(128, 512).astype(np.float16)\n",
    "\n",
    "output_nki = nki_tensor_add_kernel_(a, b)\n",
    "\n",
    "output_np = a + b\n",
    "\n",
    "allclose = np.allclose(output_np, output_nki, atol=1e-4, rtol=1e-2)\n",
    "if allclose:\n",
    "    print(\"NKI and NumPy match\")\n",
    "else:\n",
    "    print(\"NKI and NumPy differ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f65891-2d62-4af4-aa5d-7620c707f6bd",
   "metadata": {},
   "source": [
    "Now let's see if we can do that for matrix multiplication!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a65cb0-215d-4590-8335-d53c23eef5c1",
   "metadata": {},
   "source": [
    "### 3. Your second NKI kernel\n",
    "Now, let's try to use PyTorch arrays and pass them to the device with XLA. Then we'll try a matrix multiplication kernel.\n",
    "\n",
    "If you get any errors, you may need to use a different python environment.  Choose the Python 3.9 kernel (or create a new venv) and install these packages:\n",
    "\n",
    "%pip install neuronx-cc==2.18.121.0+9e31e41a\n",
    "\n",
    "%pip install torch torch_xla torch_neuronx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e24399-7bae-4db2-b964-b5fdcc93fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_xla.core import xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "lhs_small = torch.rand((64, 128), dtype=torch.bfloat16, device=device)\n",
    "rhs_small = torch.rand((128, 512), dtype=torch.bfloat16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc1f344-6e02-4f3a-928a-9f1bccabfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nki.jit\n",
    "def nki_matmul_basic_(lhsT, rhs):\n",
    "  \"\"\"NKI kernel to compute a 64x128x512 matrix multiplication operation\n",
    "\n",
    "  Args:\n",
    "      lhsT: an input tensor of shape [128,64], a left hand side argument of the\n",
    "        matrix multiplication, delivered transposed for optimal performance\n",
    "      rhs: an input tensor of shape [128,512], a right hand side argument of the\n",
    "        matrix multiplication\n",
    "  Returns:\n",
    "      result: the resulting output tensor of shape [64,512]\n",
    "  \"\"\"\n",
    "  result = nl.ndarray((64, 512), dtype=lhsT.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "  # Defining indexes for input LHS.T\n",
    "  # - Note: here we take LayoutConstraint #1 into account:\n",
    "  # \"For MatMult, contraction axis must be mapped to P-dim\"\n",
    "  i_lhsT_p, i_lhsT_f = nl.mgrid[0:128, 0:64]\n",
    "\n",
    "  # Defining indexes for input RHS\n",
    "  # - Note: here we take LayoutConstraint #1 into account:\n",
    "  # \"For MatMult, contraction axis must be mapped to P-dim\"\n",
    "  i_rhs_p, i_rhs_f = nl.mgrid[0:128, 0:512]\n",
    "\n",
    "  # Defining indexes for the output ([64,128]@[128,512] -> [64,512])\n",
    "  i_out_p, i_out_f = nl.mgrid[0:64, 0:512]\n",
    "\n",
    "  # Loading the inputs (HBM->SBUF)\n",
    "  # Note: here we take Tile dtype definition into account,\n",
    "  # which forces P-dim as the left most index\n",
    "  lhs_tile = nl.load(lhsT[i_lhsT_p, i_lhsT_f])\n",
    "  rhs_tile = nl.load(rhs[i_rhs_p, i_rhs_f])\n",
    "\n",
    "  # Perform the matrix-multiplication\n",
    "  # Note1: We set transpose_x to True, to indicate that the LHS input is transposed\n",
    "  # Note2: A NKI matmul instruction always writes to PSUM in float32 data-type\n",
    "  result_psum = nl.matmul(lhs_tile, rhs_tile, transpose_x=True)\n",
    "\n",
    "  # Copy the result from PSUM back to SBUF, and cast to expected output data-type\n",
    "  result_sbuf = nl.copy(result_psum, dtype=result.dtype)\n",
    "\n",
    "  # The result of a [64,128] x [128,512] matrix multiplication has a shape of [64, 512].\n",
    "  # This dictates which indices to use to address the result tile.\n",
    "  nl.store(result[i_out_p, i_out_f], value=result_sbuf)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b2a228-0a08-42fc-9bd4-81dedba0e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking correctness of nki_matmul_basic\n",
      "2025-03-17 22:45:04.000657:  512118  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/ec2-user/neuroncc_compile_workdir/58a5f9b5-7dd1-4569-b58f-bae92b1f0d13/model.MODULE_6255296715421101974+e30acd3a.hlo_module.pb --output /tmp/ec2-user/neuroncc_compile_workdir/58a5f9b5-7dd1-4569-b58f-bae92b1f0d13/model.MODULE_6255296715421101974+e30acd3a.neff --target=trn1 --verbose=35\n",
      ".\n",
      "Compiler status PASS\n",
      "NKI and Torch match\n"
     ]
    }
   ],
   "source": [
    "# Run NKI kernel\n",
    "output_small = nki_matmul_basic_(lhs_small.T, rhs_small)\n",
    "\n",
    "# Run torch reference\n",
    "output_small_torch = torch.matmul(lhs_small, rhs_small)\n",
    "\n",
    "# Compare results\n",
    "print(\"Checking correctness of nki_matmul_basic\")\n",
    "if torch.allclose(output_small_torch, output_small, atol=1e-4, rtol=1e-2):\n",
    "  print(\"NKI and Torch match\")\n",
    "else:\n",
    "  print(\"NKI and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801236a2-9d4d-4630-a750-dc42bb2e4514",
   "metadata": {},
   "source": [
    "### 4. Wrap up and next steps\n",
    "The simplicity you see in the `tensor_add` kernel above is possible because the shapes we pass in are very small. We've intentionally selected them to exactly match the shapes of tiles that NKI supports as maximum dimensions, for both the partition and free dimensions.\n",
    "\n",
    "As you saw above, the partition dimension has a maximum length of 128. This the most important dimension and shape to embrace in your kernels, because it impacts your ability to load data onto the chip. In order to exploit the parallelism of execution enabled through the 128 lanes on sbuf, you might want to develop into your kernel the ability to extract data in batches of 128 to load onto sbuf. \n",
    "\n",
    "The second dimension, also known as the free dimension, is more flexible. Once you have clean batches of 128 lanes being loaded onto sbuf, you can build in tiling on the second dimension of much more varying sizes up to 512. \n",
    "\n",
    "To learn more about tiling, and to step through the rest of the matrix multiplication tutorial, see our docs on the topic [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials/matrix_multiplication.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd810a4b-2365-48a3-ad0f-23f3850ffc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
