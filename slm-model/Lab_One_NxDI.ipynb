{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a972332",
   "metadata": {},
   "source": [
    "# Develop support for a new model with NeuronX Distributed Inference\n",
    "\n",
    "In this notebook you will learn how to develop support for a new model with NeuronX Distributed Inference (NxD). NxD is a Python package developed by Annapurna Labs that enables you to shard, compile, train, and host PyTorch models on Trainium and Inferentia instances. We develop two key packages demonstrating how to use this, [NxD Inference](https://github.com/aws-neuron/neuronx-distributed-inference/tree/main) and [NxD Training](https://github.com/aws-neuron/neuronx-distributed-training). This notebook focuses on inference. You will learn how to develop support for a new model in NxD Inference through the context of Llama 3.2, 1B.\n",
    "\n",
    "#### Overview\n",
    "1. Check dependencies for AWS Neuron SDK\n",
    "2. Accept the Meta usage terms and download the model from Hugging Face.\n",
    "3. Learn how to invoke the model step-by-step\n",
    "   - Load the model from a local path.\n",
    "   - Shard and compile it for Trainium.\n",
    "   - Download and tokenize the dataset\n",
    "   - Invoke the model with prompts\n",
    "4. Learn how to modify the underlying APIs to work with your own models\n",
    "\n",
    "#### Prerequisites\n",
    "This notebook was developed on a trn1.2xlarge instance, using the latest Amazon Linux DLAMI. Both the Amazon Linux and Ubuntu Neuron DLAMI's have preinstalled Python virtual environments with all the basic software packages included. The virtual environment used to develop this notebook is located at this path in both Amazon Linux and Ubuntu DLAMIs:  `/opt/aws_neuronx_venv_pytorch_2_5_nxd_inference`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652fc5a",
   "metadata": {},
   "source": [
    "### Step 1. Import NxD Inference packages\n",
    "\n",
    "If you are running this notebook in the virtual environment for NxD Inference, then the package should already be installed. Let's verify that with the following import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4405a13-5431-4d29-a6a6-2eb989fb0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronx_distributed_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1970fc",
   "metadata": {},
   "source": [
    "### Step 2. Accept the Meta usage terms and download the model\n",
    "\n",
    "If you would like to use the model directly from Meta, you'll need to navigate over to the Hugging Face hub for Llama 3.2 1B [here](https://huggingface.co/meta-llama/Llama-3.2-1B). Log in to the Hub, accept the usage term, and request access to the model. Once access has been granted, copy your Hugging Face token and paste it into the download command below.\n",
    "\n",
    "If you do not have your token readily available, you can proceed with the alternative model shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959fb008-a2c8-4505-8f60-42e5b2060b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful packages to speed up the download\n",
    "!pip install hf_transfer \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff01a8-94f7-4d10-bdf7-71229ec19cb9",
   "metadata": {},
   "source": [
    "We'll download the `NousResearch/Llama3.2-1B` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2e3d1-7c1b-4d9d-b1f5-d294a1381566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download NousResearch/Llama-3.2-1B --local-dir /home/ubuntu/environment/models/llama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02214b8a",
   "metadata": {},
   "source": [
    "### Step 3. Establish model configs\n",
    "Next, you'll point to the local model files and establish config objects. Each of these configs are helpful in successfully invoking the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e54a5f-842f-4b2c-ab79-c0f11a6ef292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original checkpoint\n",
    "model_path = '/home/ubuntu/environment/models/llama/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dc24d-dd06-45c8-adec-fa997f02e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where your NxD trace will go\n",
    "traced_model_path = '/home/ubuntu/environment/models/traced_llama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72bda4-5e04-442c-b016-f30816db54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "\n",
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "from neuronx_distributed_inference.models.llama.modeling_llama import LlamaInferenceConfig, NeuronLlamaForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "from neuronx_distributed_inference.modules.generation.sampling import prepare_sampling_params\n",
    "\n",
    "# torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812403b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the generation config to address a trailing comma\n",
    "!cp generation_config.json $model_path/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c6e49-ce3a-47c9-868a-520f0cd68276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configs \n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "# Some sample overrides for generation\n",
    "generation_config_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 1,\n",
    "    \"pad_token_id\": generation_config.eos_token_id,\n",
    "}\n",
    "generation_config.update(**generation_config_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196acdb-d094-41c0-9638-9974cec332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_config = NeuronConfig(\n",
    "    tp_degree=2,\n",
    "    batch_size=2,\n",
    "    max_context_length=32,\n",
    "    seq_len=64,\n",
    "    on_device_sampling_config=OnDeviceSamplingConfig(top_k=1),\n",
    "    enable_bucketing=True,\n",
    "    flash_decoding_enabled=False\n",
    ")\n",
    "\n",
    "# Build the Llama Inference config\n",
    "config = LlamaInferenceConfig(\n",
    "    neuron_config,\n",
    "    load_config=load_pretrained_config(model_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269bcdd-cf8c-4b10-a428-0cd0fafd83d1",
   "metadata": {},
   "source": [
    "### Step 4. Shard and compile the model\n",
    "The NeuronX compiler will optimize your model for Trainium hardware, ultimately generating the assembly code that executes your operations. We will invoke that compiler now. Generally, it's suggested to compile for some of the larger input and output shapes for your model, while using bucketing to optimize performance. Both of those are handled for you automatically with NxD.\n",
    "\n",
    "With NxD, this step also shards your checkpoint for the TP degree that you defined above. Compilation can take some time, for a 1B model this should run for a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1e5d5-a989-40fb-8350-fca737470b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = NeuronLlamaForCausalLM(model_path, config)\n",
    "model.compile(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38178c7e-0f6e-41ab-9383-2942615b82ed",
   "metadata": {},
   "source": [
    "Once compilation is complete your new model is saved and ready to load! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a37f02-ed94-4c3e-81cc-6d9e23c04175",
   "metadata": {},
   "source": [
    "### Step 5. Download the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e306f-9488-4b0e-8e6a-f238a50f2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cfe39-9e66-4a02-bf21-2560de065a34",
   "metadata": {},
   "source": [
    "### Step 6. Load the traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945db68-5392-406c-8dd6-9e66b9ab0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuronLlamaForCausalLM(traced_model_path)\n",
    "model.load(traced_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(traced_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0f5f3-b12b-4ac4-883c-856604f8d44e",
   "metadata": {},
   "source": [
    "### Step 7. Define the prompts and prepare them for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203a455-402b-4ddc-81d3-d4d1b4335c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"I believe the meaning of life is\", \"The color of the sky is\"]\n",
    "\n",
    "# Example: parameter sweeps for sampling\n",
    "sampling_params = prepare_sampling_params(batch_size=neuron_config.batch_size,\n",
    "                                         top_k=[10, 5],\n",
    "                                         top_p=[0.5, 0.9],\n",
    "                                         temperature=[0.9, 0.5])\n",
    "\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f43f8-a2a8-4986-af7c-fdc58a37f3cd",
   "metadata": {},
   "source": [
    "### Step 8. Create a Generation Adapter and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511a6f-049c-4a05-bccc-f5cce8071334",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = HuggingFaceGenerationAdapter(model)\n",
    "outputs = generation_model.generate(\n",
    "    inputs.input_ids,\n",
    "    generation_config=generation_config,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_length=model.config.neuron_config.max_length,\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "output_tokens = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"Generated outputs:\")\n",
    "for i, output_token in enumerate(output_tokens):\n",
    "    print(f\"Output {i}: {output_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b840fc-dcba-428a-bcf8-c35702d144e0",
   "metadata": {},
   "source": [
    "---\n",
    "# Develop support for a new model with NxDI\n",
    "Now that you've run inference with this model, let's take a closer look at how this works. The cells you just ran are based on a script available in our repository [here](https://github.com/aws-neuron/neuronx-distributed-inference/tree/main). You can step through this repository to understand how the objects are developed, inherited, and made available for inference. The full developer guide on the topic is available [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/onboarding-models.html#nxdi-onboarding-models). Let's look at some of the key points!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec151f-ce53-4051-a9d0-957654834f51",
   "metadata": {},
   "source": [
    "#### 1/ NeuronConfig class\n",
    "You can inherit our base `NeuronConfig` class and extend it with your own model parameters. In the notebook you just ran, this is how we defined the following parameters:\n",
    "- Tensor Parallel (TP) Degree\n",
    "- Batch size\n",
    "- Max context length (input shape)\n",
    "- Sequence length (output shape)\n",
    "- On device sampling\n",
    "- Enabling bucketing\n",
    "- Flash decoding\n",
    "\n",
    "\n",
    "This object and these parameters will be sent to the compiler when you call `model.compile`. It's a helpful way to ensure that the compiler registers your design choices so that it can start optimizations. It also enables the model sharing with NxDI for your preferred TP degree, which lets you very quickly test a variety of TP degrees (TP=8, 32, 64, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98eb22-c02b-4c74-bd4c-3cd1bd196f54",
   "metadata": {},
   "source": [
    "#### 2/ InferenceConfig class\n",
    "Next, you can inherit our base `InferenceConfig` class and extend it with the rest of your modeling parameters. In the notebook you ran above, we took two important steps with this config.\n",
    "1. Passed into it the base `NeuronConfig`.\n",
    "2. Passed the rest of the model config from the HuggingFace pretrained config.\n",
    "\n",
    "Your inference class is where you define modeling parameters like the following:\n",
    "- hidden size\n",
    "- num attention heads\n",
    "- num hidden layers\n",
    "- num key value heads\n",
    "- vocab size\n",
    "\n",
    "You'll use this `config` object to save and compile your model. Let's learn how!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71016dc5-112d-470f-a1ee-ce1855a5487d",
   "metadata": {},
   "source": [
    "#### 3/ NeuronModel\n",
    "This is how you fundamentally integrate your modeling code into the Neuron SDK. If you'd like to simply reuse our `NeuronAttentionBase`, you can inherit this directly through the library and simply pass your parameters through the `InferenceConfig` you defined above. This is how the example code in our notebook works. This is also the fastest way of getting your model online with NxD I.\n",
    "\n",
    "In the example code you ran, you also used our code for `NeuronLlamaMLP`. This is a layer in the network which inherits from `nn.Module` directly, and it's where you can define the structure of your computations. The `NeuronLlamaMLP` uses a predefined `ColumnParallelLinear` object for both the gate and up projections, while using a predefined `RowParallelLinear` object for the down projection. It also defines a forward pass on that layer.\n",
    "\n",
    "The rest of the model is defined similarly: either you inherit from our base objects and just passing in your `InferenceConfig`, or you define a new layer inheriting from `nn.Module` and write those layers as either `RowParallelLinear`, `ColumnParallelLinear`, or something else. The benefit of writing your layers into the `Row` and `Column` parallel layers as presented here is that we can handle the distribution of your model for you. \n",
    "\n",
    "For a more complete guide check out our documentation on the subject [here](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/api_guide.html#api-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98a0d4",
   "metadata": {},
   "source": [
    "### Notebook Wrap-Up\n",
    "\n",
    "For more advanced topics:\n",
    "- **Profiling**: See [Neuron Profiling Tools](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-profile/index.html).\n",
    "- **Distributed Serving**: Explore vLLM or other serving frameworks.\n",
    "- **Performance Benchmarking**: Use `llmperf` or custom scripts.\n",
    "\n",
    "Thank you for using AWS Trainium, and happy LLM experimentation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc3c62-08a4-49ae-adef-5c0d661f2712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
